{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "ep: 0, reward: 15.0, mean reward: 10.050000\n",
      "ep: 25, reward: 21.0, mean reward: 12.861675\n",
      "ep: 50, reward: 13.0, mean reward: 17.890993\n",
      "ep: 75, reward: 78.0, mean reward: 33.282602\n",
      "ep: 100, reward: 81.0, mean reward: 51.232086\n",
      "ep: 125, reward: 200.0, mean reward: 72.655471\n",
      "ep: 150, reward: 165.0, mean reward: 95.536354\n",
      "ep: 175, reward: 199.0, mean reward: 111.450592\n",
      "ep: 200, reward: 200.0, mean reward: 128.091289\n",
      "ep: 225, reward: 200.0, mean reward: 142.698325\n",
      "ep: 250, reward: 200.0, mean reward: 153.617442\n",
      "ep: 275, reward: 126.0, mean reward: 161.837093\n",
      "ep: 300, reward: 200.0, mean reward: 169.490502\n",
      "ep: 325, reward: 200.0, mean reward: 175.448844\n",
      "ep: 350, reward: 155.0, mean reward: 180.120054\n",
      "ep: 375, reward: 193.0, mean reward: 184.154317\n",
      "ep: 400, reward: 200.0, mean reward: 185.730692\n",
      "ep: 425, reward: 200.0, mean reward: 187.898415\n",
      "ep: 450, reward: 168.0, mean reward: 189.132082\n",
      "ep: 475, reward: 124.0, mean reward: 184.403100\n",
      "ep: 500, reward: 200.0, mean reward: 183.626729\n",
      "ep: 525, reward: 200.0, mean reward: 185.987445\n",
      "ep: 550, reward: 160.0, mean reward: 186.956363\n",
      "ep: 575, reward: 200.0, mean reward: 189.663913\n",
      "ep: 600, reward: 200.0, mean reward: 191.934309\n",
      "ep: 625, reward: 200.0, mean reward: 191.732311\n",
      "ep: 650, reward: 200.0, mean reward: 192.912692\n",
      "ep: 675, reward: 200.0, mean reward: 192.597706\n",
      "ep: 700, reward: 200.0, mean reward: 193.631597\n",
      "ep: 725, reward: 200.0, mean reward: 193.588639\n",
      "ep: 750, reward: 200.0, mean reward: 194.665608\n",
      "ep: 775, reward: 200.0, mean reward: 195.044987\n",
      "ep: 800, reward: 200.0, mean reward: 195.253247\n",
      "ep: 825, reward: 200.0, mean reward: 196.165226\n",
      "ep: 850, reward: 200.0, mean reward: 194.469389\n",
      "ep: 875, reward: 200.0, mean reward: 193.326604\n",
      "ep: 900, reward: 200.0, mean reward: 193.922996\n",
      "ep: 925, reward: 200.0, mean reward: 193.299805\n",
      "ep: 950, reward: 200.0, mean reward: 194.580021\n",
      "ep: 975, reward: 200.0, mean reward: 195.565598\n",
      "ep: 1000, reward: 107.0, mean reward: 190.309007\n",
      "ep: 1001: model did not converge. Try changing the hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "# hyperparameters\n",
    "n_obs = 4              # dimensionality of observations\n",
    "h = 32                # hidden layer neurons\n",
    "n_actions = 2          # number of available actions\n",
    "learning_rate = 1e-2   # how rapidly to update parameters\n",
    "gamma = .95            # reward discount factor\n",
    "decay = 0.9            # decay rate for RMSProp gradients\n",
    "\n",
    "# gamespace\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation = env.reset()\n",
    "xs,rs,ys = [],[],[]    # environment info\n",
    "running_reward = 10    # worst case is ~10 for cartpole\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "max_steps = 1000      # should converge around 300\n",
    "\n",
    "# initialize model\n",
    "tf_model = {}\n",
    "with tf.variable_scope('layer_one',reuse=False):\n",
    "    xavier_l1 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(n_obs), dtype=tf.float32)\n",
    "    tf_model['W1'] = tf.get_variable(\"W1\", [n_obs, h], initializer=xavier_l1)\n",
    "with tf.variable_scope('layer_two',reuse=False):\n",
    "    xavier_l2 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(h), dtype=tf.float32)\n",
    "    tf_model['W2'] = tf.get_variable(\"W2\", [h,n_actions], initializer=xavier_l2)\n",
    "\n",
    "# tensorflow operations\n",
    "def tf_discount_rewards(tf_r): #tf_r ~ [game_steps,1]\n",
    "    discount_f = lambda a, v: a*gamma + v;\n",
    "    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r,[True, False]))\n",
    "    tf_discounted_r = tf.reverse(tf_r_reverse,[True, False])\n",
    "    return tf_discounted_r\n",
    "\n",
    "def tf_policy_forward(x): #x ~ [1,D]\n",
    "    h = tf.matmul(x, tf_model['W1'])\n",
    "    h = tf.nn.relu(h)\n",
    "    logp = tf.matmul(h, tf_model['W2'])\n",
    "    p = tf.nn.softmax(logp)\n",
    "    return p\n",
    "\n",
    "# tf placeholders\n",
    "tf_x = tf.placeholder(dtype=tf.float32, shape=[None, n_obs],name=\"tf_x\")\n",
    "tf_y = tf.placeholder(dtype=tf.float32, shape=[None, n_actions],name=\"tf_y\")\n",
    "tf_epr = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"tf_epr\")\n",
    "\n",
    "# tf reward processing (need tf_discounted_epr for policy gradient wizardry)\n",
    "tf_discounted_epr = tf_discount_rewards(tf_epr)\n",
    "tf_mean, tf_variance= tf.nn.moments(tf_discounted_epr, [0], shift=None, name=\"reward_moments\")\n",
    "tf_discounted_epr -= tf_mean\n",
    "tf_discounted_epr /= tf.sqrt(tf_variance + 1e-6)\n",
    "\n",
    "# tf optimizer op\n",
    "tf_aprob = tf_policy_forward(tf_x)\n",
    "loss = tf.nn.l2_loss(tf_y-tf_aprob) # this gradient encourages the actions taken\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay)\n",
    "tf_grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=tf_discounted_epr)\n",
    "train_op = optimizer.apply_gradients(tf_grads)\n",
    "\n",
    "# tf graph initialization\n",
    "sess = tf.InteractiveSession()\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "# training loop\n",
    "# stop when running reward exceeds 200 (task is considered solved)\n",
    "while episode_number <= max_steps and running_reward < 200:\n",
    "    if episode_number%50==0: env.render()\n",
    "\n",
    "    # stochastically sample a policy from the network\n",
    "    x = observation\n",
    "    feed = {tf_x: np.reshape(x, (1,-1))}\n",
    "    aprob = sess.run(tf_aprob,feed)\n",
    "    aprob = aprob[0,:] # we live in a batched world :/\n",
    "    \n",
    "    action = np.random.choice(n_actions, p=aprob)\n",
    "    label = np.zeros_like(aprob) ; label[action] = 1 # make a training 'label'\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # record game history\n",
    "    xs.append(x)\n",
    "    ys.append(label)\n",
    "    rs.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        running_reward = running_reward * 0.99 + reward_sum * 0.01\n",
    "        epx = np.vstack(xs)\n",
    "        epr = np.vstack(rs)\n",
    "        epy = np.vstack(ys)\n",
    "        xs,rs,ys = [],[],[] # reset game history\n",
    "        \n",
    "        feed = {tf_x: epx, tf_epr: epr, tf_y: epy}\n",
    "        _ = sess.run(train_op,feed) # parameter update\n",
    "\n",
    "        # print some updates\n",
    "        if episode_number % 25 == 0:\n",
    "            print 'ep: {}, reward: {}, mean reward: {:3f}'.format(\n",
    "                episode_number, reward_sum, running_reward)\n",
    "        \n",
    "        # book-keeping\n",
    "        episode_number += 1\n",
    "        observation = env.reset() # reset env\n",
    "        reward_sum = 0\n",
    "\n",
    "if running_reward > 200:\n",
    "    print \"ep: {}: SOLVED! (running reward hit {} which is greater than 200)\".format(\n",
    "        episode_number, running_reward)\n",
    "else:\n",
    "    print \"ep: {}: model did not converge. Try changing the hyperparameters.\".format(episode_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
