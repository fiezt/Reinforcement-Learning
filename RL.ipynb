{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_world_mdp(object):\n",
    "    def __init__(self, grid_cols, grid_rows, actions, terminal_states=None, prob_func=None, reward_func=None):\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        \n",
    "        self.n = grid_cols * grid_rows\n",
    "        self.m = actions\n",
    "        \n",
    "        self.states = range(self.n)\n",
    "        self.actions = range(self.m)\n",
    "        \n",
    "        grid = np.zeros((self.grid_rows, self.grid_cols))\n",
    "\n",
    "        if self.m == 4:\n",
    "            \n",
    "            # Action of N, S, E, W.\n",
    "            self.actions_to_idx = {(-1,0): 0, (1,0): 1, (0,1): 2, (0,-1): 3}\n",
    "            \n",
    "            self.idx_to_actions = {v:k for k,v in self.actions_to_idx.items()}    \n",
    "            \n",
    "        elif self.m == 8:\n",
    "            \n",
    "            # Action of N, NE, NW, S, SE, SW, E, W \n",
    "            self.actions_to_idx = {(-1,0): 0, (-1,1): 1, (-1,-1): 2, (1,0): 3, \n",
    "                              (1,1): 4, (1,-1): 5, (0,1): 6, (0,-1): 7}\n",
    "            \n",
    "            self.idx_to_actions = {v:k for k,v in self.actions_to_idx.items()}\n",
    "            \n",
    "\n",
    "        self.idx_to_states = {self.states[i]:(i/self.grid_rows, i%self.grid_rows) for i in range(self.n)}\n",
    "        self.states_to_idx = {v:k for k,v in self.idx_to_states.items()}\n",
    "        \n",
    "        \n",
    "        if terminal_states is None:\n",
    "            self.terminal_states = []\n",
    "        else:\n",
    "            self.terminal_states = [0,15]\n",
    "        \n",
    "        self.create_prob_dist(prob_func)\n",
    "        \n",
    "        self.create_rewards(reward_func)\n",
    "        \n",
    "        self.check_valid_dist()\n",
    "        \n",
    "    \n",
    "    def create_prob_dist(self, prob_func=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if prob_func is None:\n",
    "            self.get_prob_dist()\n",
    "        else:\n",
    "            self.P = prob_func(self)\n",
    "        \n",
    "    \n",
    "    def get_prob_dist(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.P = np.zeros((self.n, self.m, self.n))\n",
    "        \n",
    "        for state in self.states: \n",
    "            for action in self.actions: \n",
    "                \n",
    "                if state in self.terminal_states:\n",
    "                    self.P[state, action, state] = 1\n",
    "                    continue\n",
    "\n",
    "                curr_pos = self.idx_to_states[state]\n",
    "\n",
    "                new_pos = (curr_pos[0] + self.idx_to_actions[action][0], \n",
    "                           curr_pos[1] + self.idx_to_actions[action][1])\n",
    "\n",
    "                if new_pos in self.states_to_idx:\n",
    "                    new_state = self.states_to_idx[new_pos]\n",
    "                    self.P[state, action, new_state] = 1\n",
    "\n",
    "                else:\n",
    "                    self.P[state, action, state] = 1\n",
    "        \n",
    "    \n",
    "    def create_rewards(self, reward_func=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if reward_func is None:\n",
    "            self.get_rewards()\n",
    "        else:\n",
    "            self.R = reward_func(self)\n",
    "        \n",
    "    \n",
    "    def get_rewards(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = -1*np.ones((self.n, self.m, self.n))\n",
    "\n",
    "        for state in self.terminal_states:\n",
    "            self.R[state] = 0\n",
    "    \n",
    "    \n",
    "    def check_valid_dist(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        for state in xrange(self.n):\n",
    "            for action in xrange(self.m):\n",
    "                assert abs(sum(self.P[state, action, :]) - 1) < 1e-3, 'Transitions do not sum to 1'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RL(object):\n",
    "    def __init__(self, mdp):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.mdp = mdp\n",
    "        \n",
    "    \n",
    "    def iterative_policy_evaluation(self, gamma=1):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        v = np.zeros((self.mdp.n,1))\n",
    "\n",
    "        while True:\n",
    "\n",
    "            delta = 0\n",
    "\n",
    "            for s in self.mdp.states:            \n",
    "                temp = v[s].copy()       \n",
    "\n",
    "                v[s] = sum(1/float(self.mdp.m)*sum(self.mdp.P[s, a, s_new]*(self.mdp.R[s, a, s_new] + gamma*v[s_new]) \n",
    "                                          for s_new in self.mdp.states) for a in self.mdp.actions)\n",
    "\n",
    "                delta = max(delta, abs(temp - v[s]))\n",
    "\n",
    "            if delta < .0001:\n",
    "                break\n",
    "                \n",
    "        policy = self.get_iterative_policy(v)\n",
    "        \n",
    "        return v, policy\n",
    "        \n",
    "    \n",
    "    def get_iterative_policy(self, v):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        return []\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = grid_world_mdp(4,4,8, [0,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 15]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_obj = RL(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, policy = rl_obj.iterative_policy_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ],\n",
       "       [-18.28478679],\n",
       "       [-23.4273588 ],\n",
       "       [-25.71291862],\n",
       "       [-18.28481275],\n",
       "       [-19.42762821],\n",
       "       [-22.28462841],\n",
       "       [-23.42741633],\n",
       "       [-23.42741373],\n",
       "       [-22.28465361],\n",
       "       [-19.42767162],\n",
       "       [-18.28487552],\n",
       "       [-25.71299944],\n",
       "       [-23.42746862],\n",
       "       [-18.28489896],\n",
       "       [  0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
