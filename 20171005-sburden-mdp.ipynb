{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "from numpy import linalg as la\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $(X,U,P,R)$ be an MDP, i.e.\n",
    "\n",
    "* $X$ is a finite set of $n$ *states*\n",
    "* $U$ is a finite set of $m$ *actions*\n",
    "* $P:X\\times U\\rightarrow \\Sigma(X)$ is transition probability\n",
    "* $R:X\\times U\\times X\\rightarrow \\mathbb{R}$ is cost (for reward, use $-R$)\n",
    "\n",
    "where $\\Sigma(S) = \\{p\\in[0,1]^S : \\sum_{s\\in S} p(s) = 1\\}$ is the set of probability distributions over the discrete set $S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n,m = 5,2\n",
    "X = range(n)\n",
    "U = range(m)\n",
    "# generate transition probabilities\n",
    "P = np.abs(np.random.randn(n,m,n))\n",
    "P = P / P.sum(axis=2)[...,np.newaxis]\n",
    "assert np.all(P.min(axis=2) > 0),'no probability is zero'\n",
    "assert np.all(P.max(axis=2) < 1),'no probability is one'\n",
    "assert np.allclose(P.sum(axis=2), 1.),'probabilities sum to 1'\n",
    "# generate rewards\n",
    "R = np.abs(np.random.randn(n,m,n))\n",
    "#R -= R.min(axis=2)[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with an initial state distribution $p\\in \\Sigma(X)$ and policy $\\pi:X\\rightarrow \\Sigma(U)$, can simulate by iterating:\n",
    "$$p(x)^+ = \\sum_{\\xi\\in X} p(\\xi) \\sum_{\\mu\\in U} \\pi(\\xi)(\\mu) P(\\xi,\\mu)(x).$$\n",
    "Equivalently, defining $\\Gamma : X \\rightarrow \\Sigma(X)$ by\n",
    "$$\\forall \\xi,x\\in X : \\Gamma(\\xi)(x) = \\sum_{\\mu\\in U} \\pi(\\xi)(\\mu) P(\\xi,\\mu)(x)$$\n",
    "and treating $p$ as a row vector, we can iterate $p^+ = p \\Gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial state distribution\n",
    "p0 = np.abs(np.random.randn(n)); p0 = p0 / np.sum(p0);\n",
    "assert np.allclose(np.sum(p0),1.)\n",
    "# policy\n",
    "pi = np.abs(np.random.randn(n,m)); pi = pi / np.sum(pi,axis=1)[:,np.newaxis]\n",
    "assert np.allclose(np.sum(pi,axis=1),1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterative simulation\n",
    "def sim_p(X,U,P,R,p0,pi,T=1):\n",
    "    n = len(X); m = len(U)\n",
    "    p_ = [p0]\n",
    "    for t in range(T):\n",
    "        p  = p_[-1].copy()\n",
    "        _p = np.nan*p\n",
    "        for x in X:\n",
    "            _p[x] = np.sum([[p[xi] * pi[xi,mu] * P[xi,mu,x] for mu in U] for xi in X])\n",
    "        assert np.allclose(np.sum(_p),1.),\"simulation must preserve probabilities\"\n",
    "        p_.append(_p)\n",
    "    return p_\n",
    "\n",
    "# matrix multiplication\n",
    "G = np.zeros((n,n))\n",
    "for xi in X:\n",
    "    for x in X:\n",
    "        G[xi,x] = np.sum([pi[xi,mu] * P[xi,mu,x] for mu in U])\n",
    "assert np.allclose(np.sum(G,axis=1),1.)\n",
    "\n",
    "# sanity check:  matrix multiplication yields same result as iterative simulation\n",
    "# (note:  p0 is a row vector, so we're doing left multiplication on G)\n",
    "assert np.allclose(sim_p(X,U,P,R,p0,pi)[-1], np.dot(p0,G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\Gamma$ is left-stochastic (row sums equal to 1) and ergodic (every state reachable from every other under policy $\\pi$), it has one unity eigenvalue, and all other eigenvalues are contractive (magnitude smaller than 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec Gamma = [ 1.00000000+0.j          0.13071453+0.15257207j  0.13071453-0.15257207j\n",
      " -0.19463993+0.j         -0.13540749+0.j        ]\n"
     ]
    }
   ],
   "source": [
    "print 'spec Gamma =',la.eigvals(G)\n",
    "assert np.all(np.abs(la.eigvals(G)[1:]) < 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that any initial probability distribution will asymptotically converge to the eigenvector corresponding to the unity eigenvalue, which will have no zero entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = la.eig(G.T)[1][:,0]; v /= v.sum()\n",
    "assert np.allclose(v,np.dot(v,G)),'v is left-eigvec of Gamma with eigval 1'\n",
    "assert np.allclose(v, np.dot(p0,la.matrix_power(G,100))),'p0 converges to v'\n",
    "assert np.all(v > 0.),'all states have positive probability'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider the problem of minimizing the infinite-horizon discounted cost\n",
    "$$J = \\sum_{t=0}^\\infty \\gamma^t r_t,$$\n",
    "where $\\gamma\\in(0,1)$ is a *discount factor* and $r_t = R(x_t,u_t)$ is the reward at time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any policy $\\pi : X\\rightarrow\\Sigma(U)$ has an associated *value* $V^\\pi : X\\rightarrow\\mathbb{R}$ defined by\n",
    "$$\\forall \\xi\\in X : V^\\pi(\\xi) = E[J \\mid x = \\xi]$$\n",
    "that satisfies the *Bellman equation*\n",
    "$$\\forall\\xi\\in X : V^\\pi(\\xi) = \\sum_{\\mu\\in U}\\pi(\\xi,\\mu)\\sum_{x\\in X} P(\\xi,\\mu)(x)[R(\\xi,\\mu,x) + \\gamma V^\\pi(x)].$$\n",
    "Noting that $V^\\pi$ appears linearly in the above equation, we can solve for $V^\\pi$ given $\\pi$ using linear algebra:\n",
    "$$L V^\\pi = b,$$\n",
    "$$b(\\xi) = \\sum_{x\\in X}\\sum_{\\mu\\in U} \\pi(\\xi)(\\mu) P(\\xi,\\mu)(x) R(\\xi,\\mu,x),$$\n",
    "\n",
    "$$L(\\xi,x) = \\delta(\\xi,x) - \\sum_{\\mu\\in U} g \\pi(\\xi)(\\mu) P(\\xi,\\mu)(x),$$\n",
    "where $\\delta:X\\times X\\rightarrow\\{0,1\\}$ is the *Kronecker delta*, i.e. $\\delta(\\xi,x) = 1 \\iff \\xi = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of policy:\n",
      "[ 4.98078606  4.78922657  4.41806804  4.85122071  4.54070212]\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(X,U,P,R,g,pi):\n",
    "    b = np.asarray([ np.sum([[pi[xi,mu] * P[xi,mu,x] * R[xi,mu,x] \n",
    "                          for x in X] for mu in U]) for xi in X])\n",
    "    I = np.identity(n)\n",
    "    L = I - np.asarray([ np.asarray([ np.sum([g * pi[xi,mu] * P[xi,mu,x] \n",
    "                                              for mu in U]) for x in X]) for xi in X])\n",
    "    # V satisfies Bellman equation\n",
    "    V = np.dot(b,la.inv(L).T)\n",
    "    assert np.allclose(V, [np.sum([[pi[xi,mu] * P[xi,mu,x] * (R[xi,mu,x] + g*V[x]) \n",
    "                                for x in X] for mu in U]) for xi in X])\n",
    "    return V\n",
    "print 'value of policy:'\n",
    "print policy_evaluation(X,U,P,R,g,pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the value $V^\\pi:X\\rightarrow\\mathbb{R}$ of any policy $\\pi:X\\rightarrow\\Sigma(U)$, we can perform *policy improvement* via:\n",
    "$$\\forall\\xi\\in X : \\pi'(\\xi) = \\arg\\min_{\\mu\\in U}\\sum_{x\\in X} P(\\xi,\\mu)(x)[R(\\xi,\\mu,x) + \\gamma V^\\pi(x)].$$\n",
    "(Note that there is a slight abuse of notation here since $\\pi'(\\xi)\\in\\Sigma(U)$ not $\\pi'(\\xi)\\in U$; the equation should be interpreted as specifying a deterministic policy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_improvement(X,U,P,R,g,V):\n",
    "    n = len(X); m = len(U)\n",
    "    pi = np.zeros((n,m))\n",
    "    for xi in X:\n",
    "        u = np.argmin([np.sum([P[xi,mu,x]*(R[xi,mu,x] + g*V[x]) for x in X]) for mu in U])\n",
    "        #assert len(u) == 1\n",
    "        pi[xi,u] = 1.\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improved policy:\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print 'improved policy:'\n",
    "print policy_improvement(X,U,P,R,g,policy_evaluation(X,U,P,R,g,pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively evaluating and improving the policy defines a *policy iteration* algorithm that converges to the optimal deterministic policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 iterations\n",
      "optimal policy:\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n",
      "optimal value:\n",
      "[ 4.49335097  4.30066824  4.04112354  4.43559621  4.07243505]\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(X,U,P,R,g,pi0):\n",
    "    pi_ = [pi0]\n",
    "    V_  = []\n",
    "    while len(V_) < 2 or not(np.allclose(V_[-1],V_[-2])):\n",
    "        pi = pi_[-1]\n",
    "        V  = policy_evaluation(X,U,P,R,g,pi)\n",
    "        _pi = policy_improvement(X,U,P,R,g,V)\n",
    "        pi_.append(_pi)\n",
    "        V_.append(V)\n",
    "    return pi_,V_\n",
    "\n",
    "pi_PI,V_PI = policy_iteration(X,U,P,R,g,pi)\n",
    "print '%d iterations' % len(V_PI)\n",
    "print 'optimal policy:'\n",
    "print pi_PI[-1]\n",
    "print 'optimal value:'\n",
    "print np.asarray(V_PI[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than evaluate the value of the policy, we can iterate toward the optimal policy and value simultaneously:\n",
    "$$\\forall\\xi\\in X : V_{j+1}(\\xi) = \\sum_{\\mu\\in U}\\pi_j(\\xi,\\mu)\\sum_{x\\in X} P(\\xi,\\mu)(x)[R(\\xi,\\mu,x) + \\gamma V_j(x)].$$\n",
    "$$\\forall\\xi\\in X : \\pi_{j+1}(\\xi) = \\arg\\min_{\\mu\\in U}\\sum_{x\\in X} P(\\xi,\\mu)(x)[R(\\xi,\\mu,x) + \\gamma V_{j+1}(x)].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_update(X,U,P,R,g,pi,V):\n",
    "    return np.asarray([np.sum([[pi[xi,mu] * P[xi,mu,x] * (R[xi,mu,x] + g * V[x]) \n",
    "                                for x in X] for mu in U]) for xi in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 iterations\n",
      "optimal policy:\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n",
      "optimal value:\n",
      "[ 4.4931931   4.30051037  4.04096567  4.43543835  4.07227718]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(X,U,P,R,g,pi0,maxiter=100):\n",
    "    n = len(X)\n",
    "    pi_ = [pi0]\n",
    "    V_  = [np.zeros(n)]\n",
    "    while len(V_) < 2 or (not(np.allclose(V_[-1],V_[-2])) and len(V_) < maxiter):\n",
    "        pi = pi_[-1]\n",
    "        V  = V_[-1]\n",
    "        _V = value_update(X,U,P,R,g,pi,V)\n",
    "        _pi = policy_improvement(X,U,P,R,g,_V)\n",
    "        pi_.append(_pi)\n",
    "        V_.append(_V)\n",
    "    return pi_,V_\n",
    "\n",
    "pi_VI,V_VI = value_iteration(X,U,P,R,g,pi,200)\n",
    "print '%d iterations' % len(V_VI)\n",
    "print 'optimal policy:'\n",
    "print pi_VI[-1]\n",
    "print 'optimal value:'\n",
    "print np.asarray(V_VI[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy discrepancy between VI and PI = 0\n",
      "value discrepancy between VI and PI = 1.6e-04\n"
     ]
    }
   ],
   "source": [
    "print 'policy discrepancy between VI and PI = %d'%np.max(np.abs(pi_VI[-1] - pi_PI[-1]))\n",
    "print 'value discrepancy between VI and PI = %0.1e'%np.max(np.abs(V_VI[-1] - V_PI[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anticipating future methods that learn optimal policies without access to a system model, we formulate policy and value iteration using the *quality* function (also called *action-value* function) $Q:X\\times U\\rightarrow\\mathbb{R}$ defined by:\n",
    "$$\\forall \\xi\\in X, \\mu\\in U : Q^\\pi(\\xi,\\mu) = \\sum_{x\\in X} P(\\xi,\\mu)(x)[R(\\xi,\\mu,x) + \\gamma V^\\pi(x)].$$\n",
    "If $\\pi$ is optimal, then $Q^\\pi$ is related to $V^\\pi$ and $\\pi$ via:\n",
    "$$\\forall \\xi\\in X: V^\\pi(\\xi) = \\min_{\\mu\\in U} Q^\\pi(\\xi,\\mu),\\ \\pi(\\xi) = \\arg\\min_{\\mu\\in U} Q^\\pi(\\xi,\\mu).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q function policy iteration (11.3-43,44 in LewisVrabieSyrmos2008)\n",
    "$$\\forall\\xi\\in X,\\mu\\in U : Q_j(\\xi,\\mu) = \\sum_{x\\in X} P(\\xi,\\mu)(x)[R(\\xi,\\mu,x) + \\gamma Q_j(x,\\pi_j(x))].$$\n",
    "\n",
    "$$\\forall\\xi\\in X : \\pi_{j+1}(\\xi) = \\arg\\min_{\\mu\\in U} Q_j(\\xi,\\mu).$$\n",
    "\n",
    "**This one is more annoying to implement, so I skipped it; I also can't actually make sense of the formula for a nondeterministic policy, due to the \\pi_j(x) dependence in the right-hand-side.**\n",
    "\n",
    "**Here's a proposed alternative formulation that seems to make sense for nondeterministic policies, but the corresponding code doesn't seem to work...**\n",
    "\n",
    "$$\\forall\\xi\\in X,\\mu\\in U : Q_j(\\xi,\\mu) = \\sum_{x\\in X} P(\\xi,\\mu)(x)\\left[R(\\xi,\\mu,x) + \\gamma \\sum_{u\\in U}\\pi_j(x,u) Q_j(x,u)\\right].$$\n",
    "\n",
    "$$\\forall\\xi\\in X : \\pi_{j+1}(\\xi) = \\arg\\min_{\\mu\\in U} Q_j(\\xi,\\mu).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q function value iteration (11.3-45,46 in LewisVrabieSyrmos2008)\n",
    "Just like with *policy iteration* above, iteratively evaluating and improving the policy using the $Q$ function defines a *policy iteration* algorithm:\n",
    "$$\\forall\\xi\\in X,\\mu\\in U : Q_{j+1}(\\xi,\\mu) = \\sum_{x\\in X} P(\\xi,\\mu)(x)[R(\\xi,\\mu,x) + \\gamma Q_j(x,\\pi_j(x)].$$\n",
    "\n",
    "$$\\forall\\xi\\in X : \\pi_{j+1}(\\xi) = \\arg\\min_{\\mu\\in U} Q_{j+1}(\\xi,\\mu).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_update_Q(X,U,P,R,g,pi,Q):\n",
    "    \"\"\"only correct for deterministic policies\"\"\"\n",
    "    _pi = [np.argmax(pix) for pix in pi]\n",
    "    return np.asarray([[np.sum([P[xi,mu,x] * (R[xi,mu,x] + g * Q[x,_pi[x]]) \n",
    "                                for x in X]) for mu in U] for xi in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy value:\n",
      "[ 4.49335097  4.30066824  4.04112354  4.43559621  4.07243505]\n",
      "policy value from Q:\n",
      "[ 3.86827445  3.67558904  3.41604774  3.81052325  3.44736176]\n"
     ]
    }
   ],
   "source": [
    "_pi = pi_VI[-1]\n",
    "\n",
    "print 'policy value:'\n",
    "print policy_evaluation(X,U,P,R,g,_pi)\n",
    "\n",
    "Q = np.zeros((n,m))\n",
    "for _ in range(10):\n",
    "    Q = value_update_Q(X,U,P,R,g,_pi,Q)\n",
    "print 'policy value from Q:'\n",
    "print Q.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_improvement_Q(X,U,P,R,g,Q):\n",
    "    n = len(X); m = len(U)\n",
    "    pi = np.zeros((n,m))\n",
    "    for xi in X:\n",
    "        u = np.argmin(Q[xi])\n",
    "        pi[xi,u] = 1.\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 iterations with Q\n",
      "optimal policy from Q:\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n",
      "optimal value from Q:\n",
      "[ 4.49319084  4.30050811  4.04096341  4.43543608  4.07227492]\n",
      "policy discrepancy between VI and VI with Q = 0\n",
      "value discrepancy between VI and VI with Q = 2.3e-06\n"
     ]
    }
   ],
   "source": [
    "def value_iteration_Q(X,U,P,R,g,pi0,maxiter=100):\n",
    "    n = len(X); m = len(U)\n",
    "    pi_ = [pi0]\n",
    "    Q_  = [np.zeros((n,m))]\n",
    "    while len(Q_) < 2 or (not(np.allclose(Q_[-1],Q_[-2])) and len(Q_) < maxiter):\n",
    "        pi = pi_[-1]\n",
    "        Q  = Q_[-1]\n",
    "        _Q = value_update_Q(X,U,P,R,g,pi,Q)\n",
    "        _pi = policy_improvement_Q(X,U,P,R,g,_Q)\n",
    "        pi_.append(_pi)\n",
    "        Q_.append(_Q)\n",
    "    return pi_,Q_\n",
    "\n",
    "pi_VI_Q,Q_VI = value_iteration_Q(X,U,P,R,g,pi,200)\n",
    "print '%d iterations with Q' % len(V_VI)\n",
    "print 'optimal policy from Q:'\n",
    "print pi_VI[-1]\n",
    "print 'optimal value from Q:'\n",
    "print Q_VI[-1].min(axis=1)\n",
    "\n",
    "print 'policy discrepancy between VI and VI with Q = %d'%np.max(np.abs(pi_VI[-1] - pi_VI_Q[-1]))\n",
    "print 'value discrepancy between VI and VI with Q = %0.1e'%np.max(np.abs(V_VI[-1] - Q_VI[-1].min(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 5.1 in Sutton Barto 1998\n",
    "\n",
    "When the system model (i.e. $P$ and $R$) is not known but trajectory episodes (i.e. $\\{(\\xi_t^e,\\mu_t^e,x_t^e,r_t^e)\\mid t\\in T,e\\in E\\}$) are available, can evaluate the policy using \"Monte Carlo\" methods -- simply compute the average observed discounted future reward for each state.\n",
    "\n",
    "**Note:** it's important to only include discounted future reward from the first occurence of each state in each simulation trajectory.  (Not sure if violating this slows or prevents convergence . . .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim(X,U,P,R,x0,pi,T=1):\n",
    "    n = len(X); m = len(U)\n",
    "    trj = [[None,None,x0,0.]]\n",
    "    for t in range(T):\n",
    "        xi = trj[-1][2]\n",
    "        mu = np.random.choice(U,p=pi[xi])\n",
    "        x  = np.random.choice(X,p=P[xi,mu])\n",
    "        r  = R[xi,mu,x]\n",
    "        trj.append([xi,mu,x,r])\n",
    "    return trj[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mc_policy_evaluation(X,trjs,g):\n",
    "    n = len(X)\n",
    "    V = np.zeros(n)\n",
    "    N = np.zeros(n)\n",
    "    for trj in trjs:\n",
    "        x_ = []\n",
    "        T = len(trj)\n",
    "        r_ = np.asarray(trj)[:,-1]\n",
    "        g_ = g**np.asarray(range(T))\n",
    "        for t,(xi,mu,x,r) in enumerate(trj):\n",
    "            if xi not in x_:\n",
    "                V[xi] += np.sum(r_[t:]*g_[:T-t])\n",
    "                N[xi] += 1\n",
    "                x_.append(xi)\n",
    "    return V / N\n",
    "\n",
    "def mc_policy_evaluation_Q(X,trjs,g):\n",
    "    n = len(X); m = len(U)\n",
    "    Q = np.zeros((n,m))\n",
    "    N = np.zeros((n,m))\n",
    "    for trj in trjs:\n",
    "        xu_ = []\n",
    "        T = len(trj)\n",
    "        r_ = np.asarray(trj)[:,-1]\n",
    "        g_ = g**np.asarray(range(T))\n",
    "        for t,(xi,mu,x,r) in enumerate(trj):\n",
    "            if (xi,mu) not in xu_:\n",
    "                Q[xi,mu] += np.sum(r_[t:]*g_[:T-t])\n",
    "                N[xi,mu] += 1\n",
    "                xu_.append((xi,mu))\n",
    "    return Q / (N+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E = 2000 # number of episodes\n",
    "T = 20 # horizon of episodes\n",
    "trjs = [sim(X,U,P,R,np.random.choice(X),_pi,T=T) for e in range(E)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of policy:\n",
      "[ 4.49335097  4.30066824  4.04112354  4.43559621  4.07243505]\n",
      "mc value of policy:\n",
      "[ 4.16196916  4.08145781  3.71032249  4.2224319   3.84047385]\n",
      "mc value of policy with Q:\n",
      "[ 4.15976356  4.07940888  3.70840203  4.22031539  3.83853324]\n",
      "Q:\n",
      "[[ 3.98881638  3.86827445]\n",
      " [ 3.67558904  3.78871182]\n",
      " [ 3.41604774  3.43368265]\n",
      " [ 3.89389104  3.81052325]\n",
      " [ 3.44736176  3.59903999]]\n",
      "mc Q:\n",
      "[[ 0.          4.15976356]\n",
      " [ 4.07940888  0.        ]\n",
      " [ 3.70840203  0.        ]\n",
      " [ 0.          4.22031539]\n",
      " [ 3.83853324  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print 'value of policy:'\n",
    "print policy_evaluation(X,U,P,R,g,_pi)\n",
    "print 'mc value of policy:'\n",
    "print mc_policy_evaluation(X,trjs,g)\n",
    "print 'mc value of policy with Q:'\n",
    "print mc_policy_evaluation_Q(X,trjs,g).max(axis=1) # hack b/c policy is deterministic\n",
    "print 'Q:'\n",
    "print Q\n",
    "print 'mc Q:'\n",
    "print mc_policy_evaluation_Q(X,trjs,g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a nondeterministic policy, using the same number of samples as above is clearly insufficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E = 2000 # number of episodes\n",
    "T = 20 # horizon of episodes\n",
    "trjs = [sim(X,U,P,R,np.random.choice(X),pi,T=T) for e in range(E)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of policy:\n",
      "[ 4.98078606  4.78922657  4.41806804  4.85122071  4.54070212]\n",
      "mc value of policy:\n",
      "[ 4.75160007  4.61513254  3.99250588  4.41523788  3.99179787]\n",
      "mc value of policy with Q:\n",
      "[ 4.58234905  4.56783641  3.8017087   4.13941178  3.78376539]\n"
     ]
    }
   ],
   "source": [
    "print 'value of policy:'\n",
    "print policy_evaluation(X,U,P,R,g,pi)\n",
    "print 'mc value of policy:'\n",
    "print mc_policy_evaluation(X,trjs,g)\n",
    "print 'mc value of policy with Q:'\n",
    "#print mc_policy_evaluation_Q(X,trjs,g).min(axis=1)\n",
    "print np.sum(mc_policy_evaluation_Q(X,trjs,g)*pi,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I'm currently unsure whether I'm using insufficient number or horizon of episodes, have a bug in my code, or this scheme simply doesn't converge for nondeterminstic policies.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
